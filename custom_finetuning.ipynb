{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/data1/malto/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/malto/csavelli/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        p_hall = inputs.pop(\"p(Hallucination)\")\n",
    "        cond_weights = inputs.pop(\"C-W\")\n",
    "        #cond_weights = torch.where(cond_weights > 0.5, 1.1, 0.1)\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")[:, 0]\n",
    "        loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        loss = cond_weights * loss_fn(logits, p_hall)\n",
    "        loss = loss.mean()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    \"\"\"def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys):\n",
    "        phall = inputs.pop(\"p(Hallucination)\")\n",
    "        cw = inputs.pop(\"C-W\")\n",
    "        loss, logits, labels = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "        inputs['p(Hallucination)'] = phall\n",
    "        inputs['C-W'] = cw\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        return loss, logits, labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "NUM_EPOCHS = 1\n",
    "BASE_DIR = Path(\"/data1/malto/shroom/\")\n",
    "\n",
    "FREEZE = True\n",
    "FROZEN_LAYERS = 22\n",
    "USE_SEQUENTIAL = True\n",
    "\n",
    "checkpoint = \"microsoft/deberta-v2-xlarge\"\n",
    "#checkpoint = \"microsoft/deberta-xlarge-mnli\"\n",
    "#checkpoint = \"microsoft/deberta-large-mnli\"\n",
    "#checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "#checkpoint = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples): # not batched\n",
    "    model_inputs = tokenizer(examples['hyp'], examples['tgt'] if examples['ref'] != 'src' else examples['src'], truncation=True, max_length=80)\n",
    "    model_inputs[\"labels\"] = [1 if t == \"Hallucination\" else 0 for t in examples['labels']]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    #print(eval_pred)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    #print(predictions, labels)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"Not Hallucination\", 1: \"Hallucination\"}\n",
    "label2id = {\"Not Hallucination\": 0, \"Hallucination\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.deberta.encoder.conv.conv.weight.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SEQUENTIAL:\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features=1024, out_features=2048, bias=True),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(in_features=2048, out_features=2, bias=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing...\n"
     ]
    }
   ],
   "source": [
    "if FREEZE == True and checkpoint.startswith(\"microsoft\"):\n",
    "    print(\"freezing...\")\n",
    "    for param in model.deberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.deberta.encoder.layer[:FROZEN_LAYERS].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30693/30693 [00:05<00:00, 5260.00 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['C-W', 'p(Hallucination)', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 30693\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['p(Hallucination)', 'C-W', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 501\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "# dataset manipulation\n",
    "\n",
    "syntetic_test_size_split = 0.01\n",
    "\n",
    "ds_mt = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_MT_SOLAR.model-agnostic.json\")])\n",
    "ds_dm = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_DM_SOLAR.model-agnostic.json\")])\n",
    "ds_pg = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_PG_SOLAR.model-agnostic.json\")])\n",
    "ds_val = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"val.model-agnostic.json\")])\n",
    "ds_val_aware = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"val.model-aware.json\")])\n",
    "ds_gpt = load_dataset(\"json\", data_files=str(BASE_DIR / f\"transformed_val_model_gpt.json\"))\n",
    "\n",
    "ds_mt = ds_mt.remove_columns([el for el in ds_mt['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=syntetic_test_size_split)\n",
    "ds_dm = ds_dm.remove_columns([el for el in ds_dm['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=syntetic_test_size_split)\n",
    "ds_pg = ds_pg.remove_columns([el for el in ds_pg['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=syntetic_test_size_split)\n",
    "ds_gpt = ds_gpt.remove_columns([el for el in ds_pg['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=syntetic_test_size_split)\n",
    "\n",
    "\n",
    "ds = concatenate_datasets([ds_mt['train'], ds_dm['train'], ds_pg['train'], ds_val['train'], ds_gpt['train']])\n",
    "ds = ds.shuffle()\n",
    "ds = DatasetDict({\n",
    "    'train' : ds,\n",
    "    'test' : ds_val_aware['train'],\n",
    "})\n",
    "ds = ds.map(preprocess_function)\n",
    "ds = ds.remove_columns(['hyp', 'src', 'task', 'ref', 'tgt', 'model', 'labels', 'label'])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data1/malto/shroom/checkpoint/local_model\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        #compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "trainer.label_names = []\n",
    "trainer.can_return_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': '0'}>]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([len(el) for el in ds['train']['input_ids']])\n",
    "df.hist(bins=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    predictions, _, _ = trainer.predict(ds[\"test\"])\n",
    "\n",
    "    #predictions = scipy.special.softmax(predictions, axis=-1)\n",
    "    #predictions = np.argmax(predictions, axis=-1)\n",
    "    predictions = scipy.special.expit(predictions[:, 0])\n",
    "    predictions = np.where(predictions > 0.5, 0, 1)\n",
    "\n",
    "    references = np.where(np.array(ds['test']['p(Hallucination)']) > 0.5, 0, 1)\n",
    "\n",
    "    accuracy = (predictions == references).sum() / predictions.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "accs = []\n",
    "for i in range(1):\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    print(accs)\n",
    "    trainer.train()\n",
    "    accs.append(get_accuracy())\n",
    "#clear_output(wait=True)\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "name_model = f\"{checkpoint.split('/')[-1]}_{syntetic_test_size_split}_{'frozen' if FREEZE else 'unfrozen'}_{FROZEN_LAYERS}_{BATCH_SIZE}_{NUM_EPOCHS}_{accs[-1].round(3)}_{int(100*random.random())}{'_sequential' if USE_SEQUENTIAL else ''}\"\n",
    "\n",
    "predictions, _, _ = trainer.predict(ds[\"test\"])\n",
    "predictions = scipy.special.expit(predictions[:, 0])\n",
    "predictions\n",
    "\n",
    "df = pd.DataFrame(predictions, columns=[name_model])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test['train'].map(preprocess_function_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1500/1500 [00:00<00:00, 8219.17 examples/s]\n",
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'p(Hallucination)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[1;32m      5\u001b[0m ds_test \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(BASE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.model-agnostic.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m----> 7\u001b[0m predictions, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtgt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhyp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mexpit(predictions[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m predictions\n",
      "File \u001b[0;32m/data1/malto/csavelli/venv/lib/python3.9/site-packages/transformers/trainer.py:3142\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3139\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3141\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3142\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   3144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3145\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/data1/malto/csavelli/venv/lib/python3.9/site-packages/transformers/trainer.py:3255\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3252\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3254\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3255\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3256\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3257\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data1/malto/csavelli/venv/lib/python3.9/site-packages/transformers/trainer.py:3474\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   3473\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3474\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3475\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   3477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     p_hall \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp(Hallucination)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     cond_weights \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC-W\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#cond_weights = torch.where(cond_weights > 0.5, 1.1, 0.1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# forward pass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/_collections_abc.py:904\u001b[0m, in \u001b[0;36mMutableMapping.pop\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m  If key is not found, d is returned if given, otherwise KeyError is raised.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 904\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__marker:\n",
      "File \u001b[0;32m/data1/malto/csavelli/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:253\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mwith the constraint of slice.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[item]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'p(Hallucination)'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
      "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
      "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
      "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "def preprocess_function_test(examples): # not batched\n",
    "    model_inputs = tokenizer(examples['hyp'], examples['tgt'], truncation=True, max_length=80)\n",
    "    return model_inputs\n",
    "\n",
    "ds_test = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"test.model-agnostic.json\")])\n",
    "\n",
    "predictions, _, _ = trainer.predict(ds_test['train'].map(preprocess_function_test).remove_columns([\"id\", 'tgt', 'task', 'hyp', 'src']))\n",
    "predictions = scipy.special.expit(predictions[:, 0])\n",
    "predictions\n",
    "\n",
    "df = pd.DataFrame(predictions, columns=[\"crlft_alldata_deberta_xlarge_forzen_22_sequential\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe = pd.read_csv(f\"/data1/malto/shroom/predictions_last.csv\")\n",
    "#dataframe[name_model] = df[name_model]\n",
    "#dataframe.to_csv(f\"/data1/malto/shroom/predictions_last.csv\", index=False)\n",
    "\n",
    "df.to_csv(f\"/data1/malto/shroom/crlft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset(\"json\", data_files=[str(BASE_DIR / \"test.model-agnostic.json\")])\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_test(examples): # not batched\n",
    "    model_inputs = tokenizer(examples['hyp'], examples['tgt'], truncation=True, max_length=80)\n",
    "    return model_inputs\n",
    "def add_columns(examples):\n",
    "    return {'p(Hallucination)' : 0.01, 'C-W': 1.01}\n",
    "\n",
    "ds_test = ds_test.map(preprocess_function_test).remove_columns(['tgt', 'task', 'src', 'id', 'hyp']).map(add_columns, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "predictions, _, _ = trainer.predict(ds_test[\"train\"])\n",
    "\n",
    "probabilities = scipy.special.expit(predictions[:, 0])\n",
    "predictions = np.where(probabilities > 0.5, \"Hallucination\", \"Not Hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(BASE_DIR / \"sequential.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_new = load_dataset(\"json\", data_files=[str(BASE_DIR / \"test.model-agnostic.json\")])\n",
    "\n",
    "global count\n",
    "count = 0\n",
    "def add_predictions(examples):\n",
    "    global count\n",
    "    prob = probabilities[count]\n",
    "    pred = predictions[count]\n",
    "    count += 1\n",
    "    return {'p(Hallucination)' : prob, 'label' : pred}\n",
    "ds_test_new = ds_test_new.map(add_predictions).remove_columns(['tgt', 'task', 'src', 'hyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_new['train'].to_json(str(BASE_DIR / \"submission.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label = \"a_lr1e-4\"\n",
    "\n",
    "l = [el for el in ds_test_new['train']]\n",
    "with open(BASE_DIR / f\"submission_{label}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file \n",
    "f = open(BASE_DIR / \"submission_a_lr1e-4.json\",)\n",
    "f = json.load(f)\n",
    "f1 = open(BASE_DIR / \"submission_a.json\",)\n",
    "f1 = json.load(f1)\n",
    "\n",
    "# dataframe from json file \n",
    "df = pd.DataFrame(f)\n",
    "df1 = pd.DataFrame(f1)\n",
    "df1 = df1.rename(columns={'p(Hallucination)': 'p(Hallucination)_a'})\n",
    "\n",
    "final_df = pd.concat([df, df1], axis=1)\n",
    "\n",
    "# for each row evaluate correlation between p(Hallucination) and p(Hallucination)_{label}\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(final_df['p(Hallucination)'], final_df['p(Hallucination)_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.linspace(0, 1000, 1001, dtype=int)\n",
    " \n",
    "p_hallucination = np.random.rand(1001)\n",
    "labels = np.where(p_hallucination > 0.5, \"Hallucination\", \"Not Hallucination\")\n",
    "\n",
    "i = 0 \n",
    "\n",
    "ids[0], p_hallucination[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 5 dataframes with 5 different seeds of 3 columns (id, p(Hallucination), label), where id is shared across all 5 dataframes\n",
    "import pandas as pd \n",
    "\n",
    "ids = np.linspace(0, 1000, 1001, dtype=int)\n",
    "dataframes = []\n",
    "\n",
    "for i in range(5): \n",
    "    p_hallucination = np.random.rand(1001)\n",
    "    labels = np.where(p_hallucination > 0.5, \"Hallucination\", \"Not Hallucination\")\n",
    "    df = pd.DataFrame({f\"id_{i}\": ids, f\"p(Hallucination)_{i}\": p_hallucination, f\"label_{i}\": labels})\n",
    "    dataframes.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble(dataframes: list) -> pd.DataFrame:\n",
    "    # the function works with 5 dataframes of 3 columns (id, p(Hallucination), label), where id is shared across all 5 dataframes    \n",
    "    df = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    assert df['id_0'].equals(df['id_1'])\n",
    "    assert df['id_0'].equals(df['id_2'])\n",
    "    assert df['id_0'].equals(df['id_3'])\n",
    "    assert df['id_0'].equals(df['id_4'])\n",
    "    df = df.drop(columns=['id_1', 'id_2', 'id_3', 'id_4'])\n",
    "    df.rename(columns={'id_0': 'id'}, inplace=True)\n",
    "\n",
    "    df['p(Hallucination)'] = df.loc[:, df.columns.str.startswith('p(Hallucination)')].mean(axis=1)\n",
    "    df = df.drop(columns=['p(Hallucination)_0', 'p(Hallucination)_1', 'p(Hallucination)_2', 'p(Hallucination)_3', 'p(Hallucination)_4', 'label_0', 'label_1', 'label_2', 'label_3', 'label_4'])\n",
    "    \n",
    "    df['label'] = np.where(df['p(Hallucination)'] > 0.5, \"Hallucination\", \"Not Hallucination\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ensemble = make_ensemble(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "l = [el for el in ds_ensemble]\n",
    "with open(BASE_DIR / \"ensemble_submission.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Ground Truth'] = ds_val_aware['train']['p(Hallucination)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(f\"/data1/malto/shroom/predictions_last.csv\")\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['avg'] = (dataframe['deberta-large-mnli_1_frozen_22_48_1_0.743_63_sequential'] + dataframe['deberta-v3-base_0.8_frozen_15_48_5_0.681_2']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix of dataframe \n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corrMatrix = dataframe.corr(method='spearman')\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
