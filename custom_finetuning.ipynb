{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/data1/malto/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        p_hall = inputs.pop(\"p(Hallucination)\")\n",
    "        cond_weights = inputs.pop(\"C-W\")\n",
    "        cond_weights = torch.where(cond_weights > 0.5, 1.1, 0.1)\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")[:, 0]\n",
    "        loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        loss = cond_weights * loss_fn(logits, p_hall)\n",
    "        loss = loss.mean()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    \"\"\"def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys):\n",
    "        phall = inputs.pop(\"p(Hallucination)\")\n",
    "        cw = inputs.pop(\"C-W\")\n",
    "        loss, logits, labels = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "        inputs['p(Hallucination)'] = phall\n",
    "        inputs['C-W'] = cw\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        return loss, logits, labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "BATCH_SIZE = 48\n",
    "NUM_EPOCHS = 1\n",
    "BASE_DIR = Path(\"/data1/malto/shroom/\")\n",
    "\n",
    "FREEZE = True\n",
    "FROZEN_LAYERS = 15\n",
    "\n",
    "#checkpoint = \"microsoft/deberta-xlarge-mnli\"\n",
    "checkpoint = \"microsoft/deberta-large-mnli\"\n",
    "#checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples): # not batched\n",
    "    model_inputs = tokenizer(examples['hyp'], examples['tgt'] if examples['ref'] != 'src' else examples['src'], truncation=True, max_length=80)\n",
    "    model_inputs[\"labels\"] = [1 if t == \"Hallucination\" else 0 for t in examples['labels']]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    #print(eval_pred)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    #print(predictions, labels)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"Not Hallucination\", 1: \"Hallucination\"}\n",
    "label2id = {\"Not Hallucination\": 0, \"Hallucination\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=1024, out_features=2048, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=2048, out_features=2, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FREEZE == True and checkpoint.startswith(\"microsoft\"):\n",
    "    print(\"freezing...\")\n",
    "    for param in model.deberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.deberta.encoder.layer[:FROZEN_LAYERS].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "# dataset manipulation\n",
    "ds_mt = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_MT_SOLAR.model-agnostic.json\")])\n",
    "ds_dm = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_DM_SOLAR.model-agnostic.json\")])\n",
    "ds_pg = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_PG_SOLAR.model-agnostic.json\")])\n",
    "ds_val = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"val.model-agnostic.json\")])['train'].train_test_split(test_size=0.4)\n",
    "\n",
    "ds_mt = ds_mt.remove_columns([el for el in ds_mt['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=0.4)\n",
    "ds_dm = ds_dm.remove_columns([el for el in ds_dm['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=0.4)\n",
    "ds_pg = ds_pg.remove_columns([el for el in ds_pg['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=0.4)\n",
    "\n",
    "ds = concatenate_datasets([ds_mt['train'], ds_dm['train'], ds_pg['train'], ds_val['train']])\n",
    "ds = ds.shuffle()\n",
    "ds = DatasetDict({\n",
    "    'train' : ds,\n",
    "    'test' : ds_val['test'],\n",
    "})\n",
    "ds = ds.map(preprocess_function)\n",
    "ds = ds.remove_columns(['hyp', 'src', 'task', 'ref', 'tgt', 'model', 'labels', 'label'])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data1/malto/shroom/checkpoint/local_model\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        #compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "trainer.label_names = []\n",
    "trainer.can_return_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([len(el) for el in ds['train']['input_ids']])\n",
    "df.hist(bins=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    predictions, _, _ = trainer.predict(ds[\"test\"])\n",
    "\n",
    "    #predictions = scipy.special.softmax(predictions, axis=-1)\n",
    "    #predictions = np.argmax(predictions, axis=-1)\n",
    "    predictions = scipy.special.expit(predictions[:, 0])\n",
    "    predictions = np.where(predictions > 0.5, 0, 1)\n",
    "\n",
    "    references = np.where(np.array(ds['test']['p(Hallucination)']) > 0.5, 0, 1)\n",
    "\n",
    "    accuracy = (predictions == references).sum() / predictions.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "accs = []\n",
    "for i in range(1):\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    print(accs)\n",
    "    trainer.train()\n",
    "    accs.append(get_accuracy())\n",
    "clear_output(wait=True)\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_dataset(\"json\", data_files=[str(BASE_DIR / \"test.model-agnostic.json\")])\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_test(examples): # not batched\n",
    "    model_inputs = tokenizer(examples['hyp'], examples['tgt'], truncation=True, max_length=80)\n",
    "    return model_inputs\n",
    "def add_columns(examples):\n",
    "    return {'p(Hallucination)' : 0.01, 'C-W': 1.01}\n",
    "\n",
    "ds_test = ds_test.map(preprocess_function_test).remove_columns(['tgt', 'task', 'src', 'id', 'hyp']).map(add_columns, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "predictions, _, _ = trainer.predict(ds_test[\"train\"])\n",
    "\n",
    "probabilities = scipy.special.expit(predictions[:, 0])\n",
    "predictions = np.where(probabilities > 0.5, \"Hallucination\", \"Not Hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_new = load_dataset(\"json\", data_files=[str(BASE_DIR / \"test.model-agnostic.json\")])\n",
    "\n",
    "global count\n",
    "count = 0\n",
    "def add_predictions(examples):\n",
    "    global count\n",
    "    prob = probabilities[count]\n",
    "    pred = predictions[count]\n",
    "    count += 1\n",
    "    return {'p(Hallucination)' : prob, 'label' : pred}\n",
    "ds_test_new = ds_test_new.map(add_predictions).remove_columns(['tgt', 'task', 'src', 'hyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_new['train'].to_json(str(BASE_DIR / \"submission.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "label = \"a_lr1e-4\"\n",
    "\n",
    "l = [el for el in ds_test_new['train']]\n",
    "with open(BASE_DIR / f\"submission_{label}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file \n",
    "f = open(BASE_DIR / \"submission_a_lr1e-4.json\",)\n",
    "f = json.load(f)\n",
    "f1 = open(BASE_DIR / \"submission_a.json\",)\n",
    "f1 = json.load(f1)\n",
    "\n",
    "# dataframe from json file \n",
    "df = pd.DataFrame(f)\n",
    "df1 = pd.DataFrame(f1)\n",
    "df1 = df1.rename(columns={'p(Hallucination)': 'p(Hallucination)_a'})\n",
    "\n",
    "final_df = pd.concat([df, df1], axis=1)\n",
    "\n",
    "# for each row evaluate correlation between p(Hallucination) and p(Hallucination)_{label}\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(final_df['p(Hallucination)'], final_df['p(Hallucination)_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.linspace(0, 1000, 1001, dtype=int)\n",
    " \n",
    "p_hallucination = np.random.rand(1001)\n",
    "labels = np.where(p_hallucination > 0.5, \"Hallucination\", \"Not Hallucination\")\n",
    "\n",
    "i = 0 \n",
    "\n",
    "ids[0], p_hallucination[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 5 dataframes with 5 different seeds of 3 columns (id, p(Hallucination), label), where id is shared across all 5 dataframes\n",
    "import pandas as pd \n",
    "\n",
    "ids = np.linspace(0, 1000, 1001, dtype=int)\n",
    "dataframes = []\n",
    "\n",
    "for i in range(5): \n",
    "    p_hallucination = np.random.rand(1001)\n",
    "    labels = np.where(p_hallucination > 0.5, \"Hallucination\", \"Not Hallucination\")\n",
    "    df = pd.DataFrame({f\"id_{i}\": ids, f\"p(Hallucination)_{i}\": p_hallucination, f\"label_{i}\": labels})\n",
    "    dataframes.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble(dataframes: list) -> pd.DataFrame:\n",
    "    # the function works with 5 dataframes of 3 columns (id, p(Hallucination), label), where id is shared across all 5 dataframes    \n",
    "    df = pd.concat(dataframes, axis=1)\n",
    "\n",
    "    assert df['id_0'].equals(df['id_1'])\n",
    "    assert df['id_0'].equals(df['id_2'])\n",
    "    assert df['id_0'].equals(df['id_3'])\n",
    "    assert df['id_0'].equals(df['id_4'])\n",
    "    df = df.drop(columns=['id_1', 'id_2', 'id_3', 'id_4'])\n",
    "    df.rename(columns={'id_0': 'id'}, inplace=True)\n",
    "\n",
    "    df['p(Hallucination)'] = df.loc[:, df.columns.str.startswith('p(Hallucination)')].mean(axis=1)\n",
    "    df = df.drop(columns=['p(Hallucination)_0', 'p(Hallucination)_1', 'p(Hallucination)_2', 'p(Hallucination)_3', 'p(Hallucination)_4', 'label_0', 'label_1', 'label_2', 'label_3', 'label_4'])\n",
    "    \n",
    "    df['label'] = np.where(df['p(Hallucination)'] > 0.5, \"Hallucination\", \"Not Hallucination\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ensemble = make_ensemble(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "l = [el for el in ds_ensemble]\n",
    "with open(BASE_DIR / \"ensemble_submission.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
