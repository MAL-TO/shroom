{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/data1/malto/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Quantized LLM and Few-Shot Learning to Generate Synthetic Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from pathlib import Path\n",
    "from random import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "#model_name_or_path = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "#revision = \"gptq-3bit-128g-actorder_True\"\n",
    "\n",
    "model_name_or_path = \"TheBloke/SOLAR-10.7B-Instruct-v1.0-GPTQ\"\n",
    "revision = \"gptq-8bit-32g-actorder_True\"\n",
    "\n",
    "# tasks MT, DM, PG\n",
    "TASK_TYPE = \"MT\"\n",
    "BATCH_SIZE = 40\n",
    "BASE_DIR = Path(\"/data1/malto/shroom\")\n",
    "\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-128g-actorder_True\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=revision)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token == None: # apparently Mixtral does not have a padding token in tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src'],\n",
       "    num_rows: 499\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=[\"/data1/malto/shroom/val.model-agnostic.json\"]).shuffle()\n",
    "#ds['train'] = ds['train'].filter(lambda x: x['task'] == TASK_TYPE)\n",
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\INST]\n",
      "\n",
      "Example 1\n",
      "<HYP>: Having a keen sense of humor.\n",
      "<TGT>: Acrimonious, bitter, piercing.\n",
      "<LABEL>: Hallucination\n",
      "\n",
      "Example 2\n",
      "<HYP>: (uncountable) Clothing.\n",
      "<TGT>: (uncountable) (in combination) clothing\n",
      "<LABEL>: Not Hallucination\n",
      "\n",
      "Example 3\n",
      "<HYP>: Are you certain that's the man?\n",
      "<TGT>: Can you confirm it's him?\n",
      "<LABEL>: Not Hallucination\n",
      "\n",
      "Example 4\n",
      "<HYP>: (informal) A fad.\n",
      "<TGT>: A phenomenon that becomes popular for a very short time.\n",
      "<LABEL>: Not Hallucination\n",
      "\n",
      "Example 5\n",
      "<HYP>: One square meter of solar battery produces about one watt of energy, so it’s not easy to use solar energy on a large scale right now.\n",
      "<TGT>: The output power of a one square meter solar panel is about one watt, so it is difficult to use solar power on a large scale at present.\n",
      "<LABEL>: Not Hallucination\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_examples = 5\n",
    "num_samples = len(ds['train'])\n",
    "examples = \"[INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\INST]\\n\"\n",
    "\n",
    "for i in range(num_examples):\n",
    "    num = int(random() * num_samples)\n",
    "    hyp = ds['train'][num]['hyp']\n",
    "    tgt = ds['train'][num]['tgt']\n",
    "    label = ds['train'][num]['label']\n",
    "    examples += f\"\\nExample {i+1}\\n<HYP>: {hyp}\\n<TGT>: {tgt}\\n<LABEL>: {label}\\n\"\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(mapped_ds):\n",
    "    prompts = []\n",
    "    for hyp, tgt in zip(mapped_ds[\"hyp\"], mapped_ds[\"tgt\"]):\n",
    "        prompts.append(f\"{examples}\\n Example {num_examples+1}\\n<HYP>: {hyp}\\n<TGT>: {tgt}\\n<LABEL>: \")\n",
    "    return {'prompts' : prompts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85225e60c7fd44e7a810cefc99f95602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src', 'prompts'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.map(generate_prompt, batched=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(mapped_ds):\n",
    "    input_ids = tokenizer(mapped_ds['prompts'], padding=True, truncation=True, max_length=500, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.01, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=5)\n",
    "    decoded_output = tokenizer.batch_decode(output)\n",
    "    return {'output' : decoded_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1071473d61114e2d84fccae3d5bec5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/malto/fborra/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src', 'prompts', 'output'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.map(generate_prediction, batched=True, batch_size=BATCH_SIZE)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synthetic_label(mapped_ds):\n",
    "    syn_labels = []\n",
    "    for item in mapped_ds['output']:\n",
    "        if \"Not\" in item.splitlines()[-1]:\n",
    "            syn_labels.append(\"Not Hallucination\")\n",
    "        else:\n",
    "            syn_labels.append(\"Hallucination\")\n",
    "    return {'synthetic_labels' : syn_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b30b7a87b541b687b43a97242336a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src', 'prompts', 'output', 'synthetic_labels'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.map(extract_synthetic_label, batched=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7755511022044088"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for a, b in zip(ds['train']['label'], ds['train']['synthetic_labels']):\n",
    "    if a == b:\n",
    "        correct += 1\n",
    "correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['Hallucination',\n",
       "  'Hallucination',\n",
       "  'Hallucination',\n",
       "  'Hallucination',\n",
       "  'Hallucination'],\n",
       " 'label': 'Hallucination',\n",
       " 'model': '',\n",
       " 'ref': 'tgt',\n",
       " 'hyp': 'Any of various insects of the family Mothidae, especially those of the genus Mothia.',\n",
       " 'task': 'DM',\n",
       " 'tgt': '(dated) A liver spot, especially an irregular or feathery one.',\n",
       " 'p(Hallucination)': 1.0,\n",
       " 'src': 'To remove <define> moth </define> patches , wash the spots with a solution of common bicarbonate of soda and water several times a day , until the patches are removed , which will usually be in forty - eight hours .',\n",
       " 'prompts': \"[INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\\\INST]\\n\\nExample 1\\n<HYP>: Having a keen sense of humor.\\n<TGT>: Acrimonious, bitter, piercing.\\n<LABEL>: Hallucination\\n\\nExample 2\\n<HYP>: (uncountable) Clothing.\\n<TGT>: (uncountable) (in combination) clothing\\n<LABEL>: Not Hallucination\\n\\nExample 3\\n<HYP>: Are you certain that's the man?\\n<TGT>: Can you confirm it's him?\\n<LABEL>: Not Hallucination\\n\\nExample 4\\n<HYP>: (informal) A fad.\\n<TGT>: A phenomenon that becomes popular for a very short time.\\n<LABEL>: Not Hallucination\\n\\nExample 5\\n<HYP>: One square meter of solar battery produces about one watt of energy, so it’s not easy to use solar energy on a large scale right now.\\n<TGT>: The output power of a one square meter solar panel is about one watt, so it is difficult to use solar power on a large scale at present.\\n<LABEL>: Not Hallucination\\n\\n Example 6\\n<HYP>: Any of various insects of the family Mothidae, especially those of the genus Mothia.\\n<TGT>: (dated) A liver spot, especially an irregular or feathery one.\\n<LABEL>: \",\n",
       " 'output': \"</s></s><s> [INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\\\INST]\\n\\nExample 1\\n<HYP>: Having a keen sense of humor.\\n<TGT>: Acrimonious, bitter, piercing.\\n<LABEL>: Hallucination\\n\\nExample 2\\n<HYP>: (uncountable) Clothing.\\n<TGT>: (uncountable) (in combination) clothing\\n<LABEL>: Not Hallucination\\n\\nExample 3\\n<HYP>: Are you certain that's the man?\\n<TGT>: Can you confirm it's him?\\n<LABEL>: Not Hallucination\\n\\nExample 4\\n<HYP>: (informal) A fad.\\n<TGT>: A phenomenon that becomes popular for a very short time.\\n<LABEL>: Not Hallucination\\n\\nExample 5\\n<HYP>: One square meter of solar battery produces about one watt of energy, so it’s not easy to use solar energy on a large scale right now.\\n<TGT>: The output power of a one square meter solar panel is about one watt, so it is difficult to use solar power on a large scale at present.\\n<LABEL>: Not Hallucination\\n\\n Example 6\\n<HYP>: Any of various insects of the family Mothidae, especially those of the genus Mothia.\\n<TGT>: (dated) A liver spot, especially an irregular or feathery one.\\n<LABEL>:  Hallucination\\n\\n\",\n",
       " 'synthetic_labels': 'Hallucination'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Labels for New Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_column():\n",
    "    tr_ds = load_dataset(\"json\", data_files=[str(BASE_DIR / \"train.model-agnostic.json\")])\n",
    "    tr_ds = tr_ds.map(lambda x: {'labels' : []})\n",
    "    tr_ds['train'].to_json(str(BASE_DIR / \"train_labeled_SOLAR.model-agnostic.json\"))\n",
    "\n",
    "#create_empty_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = load_dataset(\"json\", data_files=[str(BASE_DIR / \"train_labeled_SOLAR.model-agnostic.json\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31299e07886348a1bf2564c0d12ddcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_ds = tr_ds.map(generate_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed4c451ba804d55b8ca7aae761773ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_ds = tr_ds.map(generate_prediction, batched=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6b0c955da94be093f6f358bbae5e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_ds = tr_ds.map(extract_synthetic_label, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332f968cc9ce4957bdd0bff61d0eaf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hyp': \"Don't worry, it's only temporary.\",\n",
       " 'src': 'Не волнуйся. Это только временно.',\n",
       " 'task': 'MT',\n",
       " 'ref': 'either',\n",
       " 'tgt': \"Don't worry. It's only temporary.\",\n",
       " 'model': '',\n",
       " 'labels': ['Not Hallucination'],\n",
       " 'prompts': \"[INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\\\INST]\\n\\nExample 1\\n<HYP>: Having a keen sense of humor.\\n<TGT>: Acrimonious, bitter, piercing.\\n<LABEL>: Hallucination\\n\\nExample 2\\n<HYP>: (uncountable) Clothing.\\n<TGT>: (uncountable) (in combination) clothing\\n<LABEL>: Not Hallucination\\n\\nExample 3\\n<HYP>: Are you certain that's the man?\\n<TGT>: Can you confirm it's him?\\n<LABEL>: Not Hallucination\\n\\nExample 4\\n<HYP>: (informal) A fad.\\n<TGT>: A phenomenon that becomes popular for a very short time.\\n<LABEL>: Not Hallucination\\n\\nExample 5\\n<HYP>: One square meter of solar battery produces about one watt of energy, so it’s not easy to use solar energy on a large scale right now.\\n<TGT>: The output power of a one square meter solar panel is about one watt, so it is difficult to use solar power on a large scale at present.\\n<LABEL>: Not Hallucination\\n\\n Example 6\\n<HYP>: Don't worry, it's only temporary.\\n<TGT>: Don't worry. It's only temporary.\\n<LABEL>: \",\n",
       " 'output': \"</s></s></s></s></s></s><s> [INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\\\INST]\\n\\nExample 1\\n<HYP>: Having a keen sense of humor.\\n<TGT>: Acrimonious, bitter, piercing.\\n<LABEL>: Hallucination\\n\\nExample 2\\n<HYP>: (uncountable) Clothing.\\n<TGT>: (uncountable) (in combination) clothing\\n<LABEL>: Not Hallucination\\n\\nExample 3\\n<HYP>: Are you certain that's the man?\\n<TGT>: Can you confirm it's him?\\n<LABEL>: Not Hallucination\\n\\nExample 4\\n<HYP>: (informal) A fad.\\n<TGT>: A phenomenon that becomes popular for a very short time.\\n<LABEL>: Not Hallucination\\n\\nExample 5\\n<HYP>: One square meter of solar battery produces about one watt of energy, so it’s not easy to use solar energy on a large scale right now.\\n<TGT>: The output power of a one square meter solar panel is about one watt, so it is difficult to use solar power on a large scale at present.\\n<LABEL>: Not Hallucination\\n\\n Example 6\\n<HYP>: Don't worry, it's only temporary.\\n<TGT>: Don't worry. It's only temporary.\\n<LABEL>:  Not Hallucination\\n\",\n",
       " 'synthetic_labels': 'Not Hallucination'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_prediction(mapped_ds):\n",
    "    mapped_ds['labels'].append(mapped_ds['synthetic_labels'])\n",
    "    return mapped_ds\n",
    "\n",
    "tr_ds = tr_ds.map(add_prediction)\n",
    "tr_ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2d2078b4194deea71dd4c74e168013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "79891157"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ds['train'].to_json(str(BASE_DIR / \"train_labeled_SOLAR.model-agnostic.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
