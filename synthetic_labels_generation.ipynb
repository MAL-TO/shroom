{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/data1/malto/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Quantized LLM and Few-Shot Learning to Generate Synthetic Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#model_name_or_path = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
    "#revision = \"gptq-3bit-128g-actorder_True\"\n",
    "\n",
    "model_name_or_path = \"TheBloke/SOLAR-10.7B-Instruct-v1.0-GPTQ\"\n",
    "revision = \"gptq-8bit-32g-actorder_True\"\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-128g-actorder_True\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=revision)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token == None: # apparently Mixtral does not have a padding token in tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50ec070a0424aa78779b27e744cdc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src'],\n",
       "    num_rows: 187\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=[\"/data1/malto/shroom/val.model-agnostic.json\"]).shuffle()\n",
    "ds['train'] = ds['train'].filter(lambda x: x['task'] == \"MT\")\n",
    "ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\INST]\n",
      "\n",
      "Example 1\n",
      "<HYP>: I found a new book.\n",
      "<TGT>: Tom learned a new word.\n",
      "<LABEL>: Hallucination\n",
      "\n",
      "Example 2\n",
      "<HYP>: I would be Canadian!\n",
      "<TGT>: I wish I were a Canadian.\n",
      "<LABEL>: Hallucination\n",
      "\n",
      "Example 3\n",
      "<HYP>: I'm ready, aren't you?\n",
      "<TGT>: I'm ready, and you?\n",
      "<LABEL>: Not Hallucination\n",
      "\n",
      "Example 4\n",
      "<HYP>: That's unacceptable.\n",
      "<TGT>: Tom is indecisive.\n",
      "<LABEL>: Hallucination\n",
      "\n",
      "Example 5\n",
      "<HYP>: There's almost nothing I can do.\n",
      "<TGT>: I have almost nothing to do.\n",
      "<LABEL>: Not Hallucination\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "num_examples = 5\n",
    "num_samples = len(ds['train'])\n",
    "examples = \"[INST] The following are examples of pairs of hyp and tgt sentences that can either be Hallucination or Not Hallucination depending on how similar they are [\\INST]\\n\"\n",
    "\n",
    "for i in range(num_examples):\n",
    "    num = int(random() * num_samples)\n",
    "    hyp = ds['train'][num]['hyp']\n",
    "    tgt = ds['train'][num]['tgt']\n",
    "    label = ds['train'][num]['label']\n",
    "    examples += f\"\\nExample {i+1}\\n<HYP>: {hyp}\\n<TGT>: {tgt}\\n<LABEL>: {label}\\n\"\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(mapped_ds):\n",
    "    prompts = []\n",
    "    for hyp, tgt in zip(mapped_ds[\"hyp\"], mapped_ds[\"tgt\"]):\n",
    "        prompts.append(f\"{examples}\\n Example {num_examples+1}\\n<HYP>: {hyp}\\n<TGT>: {tgt}\\n<LABEL>: \")\n",
    "    return {'prompts' : prompts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8516181c3c4e4e2baf1c46024d28c32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src', 'prompts'],\n",
       "        num_rows: 187\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.map(generate_prompt, batched=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(mapped_ds):\n",
    "    input_ids = tokenizer(mapped_ds['prompts'], padding=True, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.01, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=5)\n",
    "    decoded_output = tokenizer.batch_decode(output)\n",
    "    return {'output' : decoded_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ea636f186a4e53b611df108af33e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/malto/fborra/venv/lib/python3.9/site-packages/transformers/generation/utils.py:1547: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src', 'prompts', 'output'],\n",
       "        num_rows: 187\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.map(generate_prediction, batched=True, batch_size=BATCH_SIZE)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synthetic_label(mapped_ds):\n",
    "    syn_labels = []\n",
    "    for item in mapped_ds['output']:\n",
    "        if \"Not\" in item.splitlines()[-1]:\n",
    "            syn_labels.append(\"Not Hallucination\")\n",
    "        else:\n",
    "            syn_labels.append(\"Hallucination\")\n",
    "    return {'synthetic_labels' : syn_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de714a9e00f4dd2878103558ddad6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'label', 'model', 'ref', 'hyp', 'task', 'tgt', 'p(Hallucination)', 'src', 'prompts', 'output', 'synthetic_labels'],\n",
       "        num_rows: 187\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.map(extract_synthetic_label, batched=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7967914438502673"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for a, b in zip(ds['train']['label'], ds['train']['synthetic_labels']):\n",
    "    if a == b:\n",
    "        correct += 1\n",
    "correct / num_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
