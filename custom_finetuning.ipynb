{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/data1/malto/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        p_hall = inputs.pop(\"p(Hallucination)\")\n",
    "        cond_weights = inputs.pop(\"C-W\")\n",
    "        cond_weights = torch.where(cond_weights > 0.5, 1.1, 0.1)\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")[:, 0]\n",
    "        loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        loss = cond_weights * loss_fn(logits, p_hall)\n",
    "        loss = loss.mean()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    \"\"\"def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys):\n",
    "        phall = inputs.pop(\"p(Hallucination)\")\n",
    "        cw = inputs.pop(\"C-W\")\n",
    "        loss, logits, labels = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "        inputs['p(Hallucination)'] = phall\n",
    "        inputs['C-W'] = cw\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        return loss, logits, labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "BATCH_SIZE = 48\n",
    "NUM_EPOCHS = 1\n",
    "BASE_DIR = Path(\"/data1/malto/shroom/\")\n",
    "\n",
    "FREEZE = True\n",
    "FROZEN_LAYERS = 15\n",
    "\n",
    "#checkpoint = \"microsoft/deberta-xlarge-mnli\"\n",
    "checkpoint = \"microsoft/deberta-large-mnli\"\n",
    "#checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/malto/fborra/venv/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([2, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples): # not batched\n",
    "    model_inputs = tokenizer(examples['hyp'], examples['tgt'] if examples['ref'] != 'src' else examples['src'], truncation=True, max_length=80)\n",
    "    model_inputs[\"labels\"] = [1 if t == \"Hallucination\" else 0 for t in examples['labels']]\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    #print(eval_pred)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    #print(predictions, labels)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"Not Hallucination\", 1: \"Hallucination\"}\n",
    "label2id = {\"Not Hallucination\": 0, \"Hallucination\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=1024, out_features=2048, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=2048, out_features=2, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freezing...\n"
     ]
    }
   ],
   "source": [
    "if FREEZE == True and checkpoint.startswith(\"microsoft\"):\n",
    "    print(\"freezing...\")\n",
    "    for param in model.deberta.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.deberta.encoder.layer[:FROZEN_LAYERS].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5751d365bba74aefa3d13c4e530a9208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4268342ae2444c8d3cb9900a11d3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['C-W', 'p(Hallucination)', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 18449\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['p(Hallucination)', 'C-W', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "# dataset manipulation\n",
    "ds_mt = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_MT_SOLAR.model-agnostic.json\")])\n",
    "ds_dm = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_DM_SOLAR.model-agnostic.json\")])\n",
    "ds_pg = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"train_labeled_PG_SOLAR.model-agnostic.json\")])\n",
    "ds_val = load_dataset(\"json\", data_files=[str(BASE_DIR / f\"val.model-agnostic.json\")])['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "ds_mt = ds_mt.remove_columns([el for el in ds_mt['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=0.4)\n",
    "ds_dm = ds_dm.remove_columns([el for el in ds_dm['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=0.4)\n",
    "ds_pg = ds_pg.remove_columns([el for el in ds_pg['train'].column_names if el not in ds_val['train'].column_names])['train'].train_test_split(test_size=0.4)\n",
    "\n",
    "ds = concatenate_datasets([ds_mt['train'], ds_dm['train'], ds_pg['train'], ds_val['train']])\n",
    "ds = ds.shuffle(seed=42)\n",
    "ds = DatasetDict({\n",
    "    'train' : ds,\n",
    "    'test' : ds_val['test'],\n",
    "})\n",
    "ds = ds.map(preprocess_function)\n",
    "ds = ds.remove_columns(['hyp', 'src', 'task', 'ref', 'tgt', 'model', 'labels', 'label'])\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data1/malto/shroom/checkpoint/local_model\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        #compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "trainer.label_names = []\n",
    "trainer.can_return_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': '0'}>]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGzCAYAAAAxPS2EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3QklEQVR4nO3de3RU9b3//9ckJBOCJCFwkpA2YGot94sFCfGuhESgXpCqHFIbKwdOaWLFtAhpBQOoSPQgl1I59CjYJWnVtlBFCoygREvkEkm5yAJsUVxikq7GMAJlGJL9+8Nf9tchCSYwk8ln5vlYaxbMZ39m7/c7ewwv92XGYVmWJQAAAINEBLsAAACAtiLAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAGMPj8WjmzJlKTU1V586dlZGRIZfLFeyyAAQBAQaAMe6//34tWrRIubm5WrJkiSIjIzV27Fi9++67wS4NQDtz8GWOAEywc+dOZWRk6Omnn9bPf/5zSdKZM2c0cOBAJSUlafv27UGuEEB74ggMACP84Q9/UGRkpKZOnWqPxcTEaPLkySovL9cnn3wSxOoAtDcCDAAj7NmzR9/5zncUFxfnMz5ixAhJUmVlZRCqAhAsBBgARvjss8/Us2fPJuONY8ePH2/vkgAEEQEGgBH+/e9/y+l0NhmPiYmxlwMIHwQYAEbo3LmzPB5Pk/EzZ87YywGEDwIMACP07NlTn332WZPxxrHU1NT2LglAEBFgABhh6NChOnz4sNxut8/4jh077OUAwgcBBoARvv/976u+vl4rV660xzwej1atWqWMjAylpaUFsToA7a1TsAsAgNbIyMjQ3XffraKiItXU1Ojb3/62XnzxRX300Ud6/vnng10egHbGJ/ECMMaZM2c0e/ZsvfTSS/r88881ePBgzZ8/Xzk5OcEuDUA7I8AAAADjcA0MAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxQvaD7BoaGnT8+HF17dpVDocj2OUAAIBWsCxLX3zxhVJTUxUR0fJxlpANMMePH+ejxQEAMNQnn3yib37zmy0uD9kA07VrV0lf/gDi4uKCXE3zvF6vNm/erOzsbEVFRQW7nHZD3+HTdzj2LNE3fYe+QPbsdruVlpZm/zvekpANMI2njeLi4jp0gImNjVVcXFzYvOkl+g6nvsOxZ4m+6Tv0tUfPX3f5BxfxAgAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABinU7ALQPu4fNYbzY5/9NS4dq4EAIBLxxEYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHHaHGDKysp02223KTU1VQ6HQ+vWrWsy5+DBg7r99tsVHx+vLl266Oqrr9axY8fs5WfOnFF+fr66d++uyy67TBMmTFB1dbXPOo4dO6Zx48YpNjZWSUlJmjFjhs6dO9f2DgEAQMhpc4A5deqUhgwZouXLlze7/O9//7uuu+469e3bV2+//bb27t2r2bNnKyYmxp7z8MMP6/XXX9err76qbdu26fjx47rrrrvs5fX19Ro3bpzOnj2r7du368UXX9Tq1as1Z86ci2gRAACEmk5tfcGYMWM0ZsyYFpf/8pe/1NixY1VSUmKPXXHFFfbfT5w4oeeff16lpaW65ZZbJEmrVq1Sv3799N5772nkyJHavHmzPvjgA7355ptKTk7W0KFDNX/+fM2cOVPFxcWKjo5ua9kAACCEtDnAXEhDQ4PeeOMNPfLII8rJydGePXuUnp6uoqIi3XnnnZKkiooKeb1eZWVl2a/r27evevXqpfLyco0cOVLl5eUaNGiQkpOT7Tk5OTmaNm2aDhw4oKuuuqrJtj0ejzwej/3c7XZLkrxer7xerz/b9JvGutqjPmekdcEa2lN79t2RhGPf4dizRN/0HfoC2XNr1+nXAFNTU6OTJ0/qqaee0uOPP66FCxdq48aNuuuuu/TWW2/pxhtvVFVVlaKjo5WQkODz2uTkZFVVVUmSqqqqfMJL4/LGZc1ZsGCB5s6d22R88+bNio2N9UN3geNyuQK+jZIRzY9v2LAh4NtuSXv03RGFY9/h2LNE3+EmHPsORM+nT59u1Ty/H4GRpDvuuEMPP/ywJGno0KHavn27VqxYoRtvvNGfm/NRVFSkwsJC+7nb7VZaWpqys7MVFxcXsO1eCq/XK5fLpdGjRysqKiqg2xpYvKnZ8f3FOQHdbnPas++OJBz7DseeJfqm79AXyJ4bz6B8Hb8GmB49eqhTp07q37+/z3i/fv307rvvSpJSUlJ09uxZ1dXV+RyFqa6uVkpKij1n586dPutovEupcc75nE6nnE5nk/GoqKgO/4Zqjxo99Y4Wtx0sJuybQAjHvsOxZ4m+w0049h2Inlu7Pr9+Dkx0dLSuvvpqHTp0yGf88OHD6t27tyRp2LBhioqK0pYtW+zlhw4d0rFjx5SZmSlJyszM1L59+1RTU2PPcblciouLaxKOAABA+GnzEZiTJ0/qww8/tJ8fPXpUlZWVSkxMVK9evTRjxgzde++9uuGGG3TzzTdr48aNev311/X2229LkuLj4zV58mQVFhYqMTFRcXFxevDBB5WZmamRI0dKkrKzs9W/f3/dd999KikpUVVVlR599FHl5+c3e5QFAACElzYHmN27d+vmm2+2nzded5KXl6fVq1dr/PjxWrFihRYsWKCf/vSn6tOnj/74xz/quuuus1/z7LPPKiIiQhMmTJDH41FOTo5+/etf28sjIyO1fv16TZs2TZmZmerSpYvy8vI0b968S+kVAACEiDYHmJtuukmW1fwtuY0eeOABPfDAAy0uj4mJ0fLly1v8MDxJ6t27d1DvkAEAAB0X34UEAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYJxOwS4AF+/yWW80GfvoqXFBqAQAgPbFERgAAGAcAgwAADAOAQYAABiHa2DQalxzAwDoKDgCAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjtDnAlJWV6bbbblNqaqocDofWrVvX4twf//jHcjgcWrx4sc94bW2tcnNzFRcXp4SEBE2ePFknT570mbN3715df/31iomJUVpamkpKStpaKgAACFFtDjCnTp3SkCFDtHz58gvOW7t2rd577z2lpqY2WZabm6sDBw7I5XJp/fr1Kisr09SpU+3lbrdb2dnZ6t27tyoqKvT000+ruLhYK1eubGu5AAAgBLX526jHjBmjMWPGXHDOp59+qgcffFCbNm3SuHG+31Z88OBBbdy4Ubt27dLw4cMlScuWLdPYsWP1zDPPKDU1VWvWrNHZs2f1wgsvKDo6WgMGDFBlZaUWLVrkE3QAAEB4anOA+ToNDQ267777NGPGDA0YMKDJ8vLyciUkJNjhRZKysrIUERGhHTt2aPz48SovL9cNN9yg6Ohoe05OTo4WLlyozz//XN26dWuyXo/HI4/HYz93u92SJK/XK6/X688W/aaxroutzxlptbjO1sxt67bbsr0LudS+TRWOfYdjzxJ903foC2TPrV2n3wPMwoUL1alTJ/30pz9tdnlVVZWSkpJ8i+jUSYmJiaqqqrLnpKen+8xJTk62lzUXYBYsWKC5c+c2Gd+8ebNiY2Mvqpf24nK5Lup1JSOajm3YsKHVcy80/1K31xoX27fpwrHvcOxZou9wE459B6Ln06dPt2qeXwNMRUWFlixZovfff18Oh8Ofq/5aRUVFKiwstJ+73W6lpaUpOztbcXFx7VpLa3m9XrlcLo0ePVpRUVFtfv3A4k1NxvYX57R67oXmX+r2LuRS+zZVOPYdjj1L9E3foS+QPTeeQfk6fg0w77zzjmpqatSrVy97rL6+Xj/72c+0ePFiffTRR0pJSVFNTY3P686dO6fa2lqlpKRIklJSUlRdXe0zp/F545zzOZ1OOZ3OJuNRUVEd/g11sTV66puGxJbW09zcC82/1O21hgn7JhDCse9w7Fmi73ATjn0HoufWrs+vnwNz3333ae/evaqsrLQfqampmjFjhjZt+vL/3jMzM1VXV6eKigr7dVu3blVDQ4MyMjLsOWVlZT7nwVwul/r06dPs6SMAABBe2nwE5uTJk/rwww/t50ePHlVlZaUSExPVq1cvde/e3Wd+VFSUUlJS1KdPH0lSv379dOutt2rKlClasWKFvF6vCgoKNHHiRPuW60mTJmnu3LmaPHmyZs6cqf3792vJkiV69tlnL6VXAAAQItocYHbv3q2bb77Zft543UleXp5Wr17dqnWsWbNGBQUFGjVqlCIiIjRhwgQtXbrUXh4fH6/NmzcrPz9fw4YNU48ePTRnzhxuoQYAAJIuIsDcdNNNsqzmb8ltzkcffdRkLDExUaWlpRd83eDBg/XOO++0tTwAABAG+C4kAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBx2hxgysrKdNtttyk1NVUOh0Pr1q2zl3m9Xs2cOVODBg1Sly5dlJqaqh/+8Ic6fvy4zzpqa2uVm5uruLg4JSQkaPLkyTp58qTPnL179+r6669XTEyM0tLSVFJScnEdAgCAkNPmAHPq1CkNGTJEy5cvb7Ls9OnTev/99zV79my9//77+tOf/qRDhw7p9ttv95mXm5urAwcOyOVyaf369SorK9PUqVPt5W63W9nZ2erdu7cqKir09NNPq7i4WCtXrryIFgEAQKjp1NYXjBkzRmPGjGl2WXx8vFwul8/Yr371K40YMULHjh1Tr169dPDgQW3cuFG7du3S8OHDJUnLli3T2LFj9cwzzyg1NVVr1qzR2bNn9cILLyg6OloDBgxQZWWlFi1a5BN0AABAeGpzgGmrEydOyOFwKCEhQZJUXl6uhIQEO7xIUlZWliIiIrRjxw6NHz9e5eXluuGGGxQdHW3PycnJ0cKFC/X555+rW7duTbbj8Xjk8Xjs5263W9KXp7W8Xm+Aurs0jXVdbH3OSKvFdbZmblu33ZbtXcil9m2qcOw7HHuW6Ju+Q18ge27tOgMaYM6cOaOZM2fqP//zPxUXFydJqqqqUlJSkm8RnTopMTFRVVVV9pz09HSfOcnJyfay5gLMggULNHfu3CbjmzdvVmxsrF/6CZTzj1q1VsmIpmMbNmxo9dwLzb/U7bXGxfZtunDsOxx7lug73IRj34Ho+fTp062aF7AA4/V6dc8998iyLD333HOB2oytqKhIhYWF9nO32620tDRlZ2fb4amj8Xq9crlcGj16tKKiotr8+oHFm5qM7S/OafXcC82/1O1dyKX2bapw7Dsce5bom75DXyB7bjyD8nUCEmAaw8vHH3+srVu3+gSIlJQU1dTU+Mw/d+6camtrlZKSYs+prq72mdP4vHHO+ZxOp5xOZ5PxqKioDv+GutgaPfWOZtfV2rkXmn+p22sNE/ZNIIRj3+HYs0Tf4SYc+w5Ez61dn98/B6YxvBw5ckRvvvmmunfv7rM8MzNTdXV1qqiosMe2bt2qhoYGZWRk2HPKysp8zoO5XC716dOn2dNHAAAgvLT5CMzJkyf14Ycf2s+PHj2qyspKJSYmqmfPnvr+97+v999/X+vXr1d9fb19XUtiYqKio6PVr18/3XrrrZoyZYpWrFghr9ergoICTZw4UampqZKkSZMmae7cuZo8ebJmzpyp/fv3a8mSJXr22Wf91HbounzWG8EuAQCAgGtzgNm9e7duvvlm+3njdSd5eXkqLi7Wa6+9JkkaOnSoz+veeust3XTTTZKkNWvWqKCgQKNGjVJERIQmTJigpUuX2nPj4+O1efNm5efna9iwYerRo4fmzJnDLdQAAEDSRQSYm266SZbV/C25ki64rFFiYqJKS0svOGfw4MF655132loeAAAIA3wXEgAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwTkC+jRpo7juZPnpqXBAqAQCEIo7AAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAME6bA0xZWZluu+02paamyuFwaN26dT7LLcvSnDlz1LNnT3Xu3FlZWVk6cuSIz5za2lrl5uYqLi5OCQkJmjx5sk6ePOkzZ+/evbr++usVExOjtLQ0lZSUtL07AAAQktocYE6dOqUhQ4Zo+fLlzS4vKSnR0qVLtWLFCu3YsUNdunRRTk6Ozpw5Y8/Jzc3VgQMH5HK5tH79epWVlWnq1Kn2crfbrezsbPXu3VsVFRV6+umnVVxcrJUrV15Ei7iQy2e90eQBAEBH16mtLxgzZozGjBnT7DLLsrR48WI9+uijuuOOOyRJv/3tb5WcnKx169Zp4sSJOnjwoDZu3Khdu3Zp+PDhkqRly5Zp7NixeuaZZ5Samqo1a9bo7NmzeuGFFxQdHa0BAwaosrJSixYt8gk6X+XxeOTxeOznbrdbkuT1euX1etvaZrtorOti63NGWv4sx9ZSPc1t72LmXmrfpgrHvsOxZ4m+6Tv0BbLn1q7TYVnWRf8r6HA4tHbtWt15552SpH/84x+64oortGfPHg0dOtSed+ONN2ro0KFasmSJXnjhBf3sZz/T559/bi8/d+6cYmJi9Oqrr2r8+PH64Q9/KLfb7XN66q233tItt9yi2tpadevWrUktxcXFmjt3bpPx0tJSxcbGXmyLAACgHZ0+fVqTJk3SiRMnFBcX1+K8Nh+BuZCqqipJUnJyss94cnKyvayqqkpJSUm+RXTqpMTERJ856enpTdbRuKy5AFNUVKTCwkL7udvtVlpamrKzsy/4Awgmr9crl8ul0aNHKyoqqs2vH1i8KQBVSfuLc1q9vYuZe6l9myoc+w7HniX6pu/QF8ieG8+gfB2/BphgcjqdcjqdTcajoqI6/BvqYmv01DsCUI1arKW57V3KXBP2TSCEY9/h2LNE3+EmHPsORM+tXZ9fb6NOSUmRJFVXV/uMV1dX28tSUlJUU1Pjs/zcuXOqra31mdPcOr66DQAAEL78GmDS09OVkpKiLVu22GNut1s7duxQZmamJCkzM1N1dXWqqKiw52zdulUNDQ3KyMiw55SVlflcyONyudSnT59mTx8BAIDw0uYAc/LkSVVWVqqyslKSdPToUVVWVurYsWNyOByaPn26Hn/8cb322mvat2+ffvjDHyo1NdW+0Ldfv3669dZbNWXKFO3cuVN//etfVVBQoIkTJyo1NVWSNGnSJEVHR2vy5Mk6cOCAXn75ZS1ZssTnGhcAABC+2nwNzO7du3XzzTfbzxtDRV5enlavXq1HHnlEp06d0tSpU1VXV6frrrtOGzduVExMjP2aNWvWqKCgQKNGjVJERIQmTJigpUuX2svj4+O1efNm5efna9iwYerRo4fmzJnT4i3UAAAgvLQ5wNx000260J3XDodD8+bN07x581qck5iYqNLS0gtuZ/DgwXrnnXfaWh4AAAgDfBcSAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxukU7ALg6/JZbzQZ++ipcUGoBACAjosjMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxuE2agRdc7eOS9w+DgBoGQEGl6Sl8AEAQCBxCgkAABjH7wGmvr5es2fPVnp6ujp37qwrrrhC8+fPl2VZ9hzLsjRnzhz17NlTnTt3VlZWlo4cOeKzntraWuXm5iouLk4JCQmaPHmyTp486e9yAQCAgfweYBYuXKjnnntOv/rVr3Tw4EEtXLhQJSUlWrZsmT2npKRES5cu1YoVK7Rjxw516dJFOTk5OnPmjD0nNzdXBw4ckMvl0vr161VWVqapU6f6u1wAAGAgv18Ds337dt1xxx0aN+7LCzAvv/xy/e53v9POnTslfXn0ZfHixXr00Ud1xx13SJJ++9vfKjk5WevWrdPEiRN18OBBbdy4Ubt27dLw4cMlScuWLdPYsWP1zDPPKDU11d9lAwAAg/g9wFxzzTVauXKlDh8+rO985zv629/+pnfffVeLFi2SJB09elRVVVXKysqyXxMfH6+MjAyVl5dr4sSJKi8vV0JCgh1eJCkrK0sRERHasWOHxo8f32S7Ho9HHo/Hfu52uyVJXq9XXq/X3236RWNdX63PGWm1OO98zc31Z13+3t75/Tb+2dJ6O+p+u1jN7e9QF449S/RN36EvkD23dp0O66sXp/hBQ0ODfvGLX6ikpESRkZGqr6/XE088oaKiIklfHqG59tprdfz4cfXs2dN+3T333COHw6GXX35ZTz75pF588UUdOnTIZ91JSUmaO3eupk2b1mS7xcXFmjt3bpPx0tJSxcbG+rNFAAAQIKdPn9akSZN04sQJxcXFtTjP70dgXnnlFa1Zs0alpaUaMGCAKisrNX36dKWmpiovL8/fm7MVFRWpsLDQfu52u5WWlqbs7OwL/gCCyev1yuVyafTo0YqKipIkDSze1GTe/uKcZl/f3Fx/CNT2Gtd7ft8trbelOkzV3P4OdeHYs0Tf9B36Atlz4xmUr+P3ADNjxgzNmjVLEydOlCQNGjRIH3/8sRYsWKC8vDylpKRIkqqrq32OwFRXV2vo0KGSpJSUFNXU1Pis99y5c6qtrbVffz6n0ymn09lkPCoqqsO/ob5ao6fe0ezy5jQ311/1BGJ756+3se+W1tvR99vFMuE96W/h2LNE3+EmHPsORM+tXZ/fA8zp06cVEeF7c1NkZKQaGhokSenp6UpJSdGWLVvswOJ2u7Vjxw771FBmZqbq6upUUVGhYcOGSZK2bt2qhoYGZWRk+LvkDo8PiwMAwJffA8xtt92mJ554Qr169dKAAQO0Z88eLVq0SA888IAkyeFwaPr06Xr88cd15ZVXKj09XbNnz1ZqaqruvPNOSVK/fv106623asqUKVqxYoW8Xq8KCgo0ceJE7kACAAD+DzDLli3T7Nmz9ZOf/EQ1NTVKTU3Vf//3f2vOnDn2nEceeUSnTp3S1KlTVVdXp+uuu04bN25UTEyMPWfNmjUqKCjQqFGjFBERoQkTJmjp0qX+LhcAABjI7wGma9euWrx4sRYvXtziHIfDoXnz5mnevHktzklMTFRpaam/ywMAACGA70ICAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjdAp2AQgfl896Q5LkjLRUMkIaWLxJnnpHkKsCAJiIIzAAAMA4BBgAAGAcAgwAADAOAQYAABgnIAHm008/1Q9+8AN1795dnTt31qBBg7R79257uWVZmjNnjnr27KnOnTsrKytLR44c8VlHbW2tcnNzFRcXp4SEBE2ePFknT54MRLkAAMAwfg8wn3/+ua699lpFRUXpL3/5iz744AP9z//8j7p162bPKSkp0dKlS7VixQrt2LFDXbp0UU5Ojs6cOWPPyc3N1YEDB+RyubR+/XqVlZVp6tSp/i4XAAAYyO+3US9cuFBpaWlatWqVPZaenm7/3bIsLV68WI8++qjuuOMOSdJvf/tbJScna926dZo4caIOHjyojRs3ateuXRo+fLgkadmyZRo7dqyeeeYZpaam+rtsAABgEL8HmNdee005OTm6++67tW3bNn3jG9/QT37yE02ZMkWSdPToUVVVVSkrK8t+TXx8vDIyMlReXq6JEyeqvLxcCQkJdniRpKysLEVERGjHjh0aP358k+16PB55PB77udvtliR5vV55vV5/t+kXjXV9tT5npBWscmwt/bz8VZszwvL5s611mKq5/R3qwrFnib7pO/QFsufWrtNhWZZf/8WMiYmRJBUWFuruu+/Wrl279NBDD2nFihXKy8vT9u3bde211+r48ePq2bOn/bp77rlHDodDL7/8sp588km9+OKLOnTokM+6k5KSNHfuXE2bNq3JdouLizV37twm46WlpYqNjfVniwAAIEBOnz6tSZMm6cSJE4qLi2txnt+PwDQ0NGj48OF68sknJUlXXXWV9u/fbweYQCkqKlJhYaH93O12Ky0tTdnZ2Rf8AQST1+uVy+XS6NGjFRUVJenLT6cNtv3FOc2O+6s2Z4Sl+cMbNHt3hDwNLX8Sb0t1mKq5/R3qwrFnib7pO/QFsufGMyhfx+8BpmfPnurfv7/PWL9+/fTHP/5RkpSSkiJJqq6u9jkCU11draFDh9pzampqfNZx7tw51dbW2q8/n9PplNPpbDIeFRXV4d9QX62xI3y0fks/L3/X5mlwXHCdHX2/XSwT3pP+Fo49S/QdbsKx70D03Nr1+f0upGuvvbbJqZ/Dhw+rd+/ekr68oDclJUVbtmyxl7vdbu3YsUOZmZmSpMzMTNXV1amiosKes3XrVjU0NCgjI8PfJQMAAMP4/QjMww8/rGuuuUZPPvmk7rnnHu3cuVMrV67UypUrJUkOh0PTp0/X448/riuvvFLp6emaPXu2UlNTdeedd0r68ojNrbfeqilTpmjFihXyer0qKCjQxIkTuQMJAAD4P8BcffXVWrt2rYqKijRv3jylp6dr8eLFys3Ntec88sgjOnXqlKZOnaq6ujpdd9112rhxo30BsCStWbNGBQUFGjVqlCIiIjRhwgQtXbrU3+XCMI3faH2+j54a186VAACCye8BRpK+973v6Xvf+16Lyx0Oh+bNm6d58+a1OCcxMVGlpaWBKA8AABiO70ICAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYJyAfBs14A+Xz3oj2CUAADoojsAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDh8FxJCVkvfpfTRU+PauRIAgL9xBAYAABiHAAMAAIxDgAEAAMYhwAAAAOMEPMA89dRTcjgcmj59uj125swZ5efnq3v37rrssss0YcIEVVdX+7zu2LFjGjdunGJjY5WUlKQZM2bo3LlzgS4Xhrp81htNHgCA0BXQALNr1y797//+rwYPHuwz/vDDD+v111/Xq6++qm3btun48eO666677OX19fUaN26czp49q+3bt+vFF1/U6tWrNWfOnECWCwAADBGwAHPy5Enl5ubqN7/5jbp162aPnzhxQs8//7wWLVqkW265RcOGDdOqVau0fft2vffee5KkzZs364MPPtBLL72koUOHasyYMZo/f76WL1+us2fPBqpkAABgiIB9Dkx+fr7GjRunrKwsPf744/Z4RUWFvF6vsrKy7LG+ffuqV69eKi8v18iRI1VeXq5BgwYpOTnZnpOTk6Np06bpwIEDuuqqq5psz+PxyOPx2M/dbrckyev1yuv1BqLFS9ZY11frc0ZawSrH1tLPy1+1OSMsnz/bW7DeD83t71AXjj1L9E3foS+QPbd2nQEJML///e/1/vvva9euXU2WVVVVKTo6WgkJCT7jycnJqqqqsud8Nbw0Lm9c1pwFCxZo7ty5TcY3b96s2NjYi2mj3bhcLvvvJSOCWMj/b8OGDc2O+7u2+cMb/LvCVmqpv/by1f0dLsKxZ4m+w0049h2Ink+fPt2qeX4PMJ988okeeughuVwuxcTE+Hv1LSoqKlJhYaH93O12Ky0tTdnZ2YqLi2u3OtrC6/XK5XJp9OjRioqKkiQNLN4U5Kqk/cU5zY77qzZnhKX5wxs0e3eEPA0Ov6yzLVrqL9Ca29+hLhx7luibvkNfIHtuPIPydfweYCoqKlRTU6Pvfve79lh9fb3Kysr0q1/9Sps2bdLZs2dVV1fncxSmurpaKSkpkqSUlBTt3LnTZ72Ndyk1zjmf0+mU0+lsMh4VFdXh31BfrdFT3/7/oJ+vpZ+Xv2vzNDiC0m+w3w8mvCf9LRx7lug73IRj34HoubXr8/tFvKNGjdK+fftUWVlpP4YPH67c3Fz771FRUdqyZYv9mkOHDunYsWPKzMyUJGVmZmrfvn2qqamx57hcLsXFxal///7+LhkAABjG70dgunbtqoEDB/qMdenSRd27d7fHJ0+erMLCQiUmJiouLk4PPvigMjMzNXLkSElSdna2+vfvr/vuu08lJSWqqqrSo48+qvz8/GaPsgAAgPASlG+jfvbZZxUREaEJEybI4/EoJydHv/71r+3lkZGRWr9+vaZNm6bMzEx16dJFeXl5mjdvXjDKBQAAHUy7BJi3337b53lMTIyWL1+u5cuXt/ia3r17B/1uEQAA0DHxXUgAAMA4QTmFBHQ0LX130kdPjWvnSgAArcERGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA43AbNXAB3F4NAB0TR2AAAIBxCDAAAMA4BBgAAGAcroEBLkJz18ZwXQwAtB+OwAAAAONwBCZILp/1hpyRlkpGSAOLN8lT7wh2SQAAGIMjMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxuE2asBP+OJHAGg/HIEBAADGIcAAAADjEGAAAIBxuAYGCLDGa2O++tURh574XpCrAgCzcQQGAAAYhwADAACMQ4ABAADGIcAAAADjcBEvEAR86B0AXBq/H4FZsGCBrr76anXt2lVJSUm68847dejQIZ85Z86cUX5+vrp3767LLrtMEyZMUHV1tc+cY8eOady4cYqNjVVSUpJmzJihc+fO+btcAABgIL8HmG3btik/P1/vvfeeXC6XvF6vsrOzderUKXvOww8/rNdff12vvvqqtm3bpuPHj+uuu+6yl9fX12vcuHE6e/astm/frhdffFGrV6/WnDlz/F0uAAAwkN9PIW3cuNHn+erVq5WUlKSKigrdcMMNOnHihJ5//nmVlpbqlltukSStWrVK/fr103vvvaeRI0dq8+bN+uCDD/Tmm28qOTlZQ4cO1fz58zVz5kwVFxcrOjra32UHVEunCwAAwMUJ+DUwJ06ckCQlJiZKkioqKuT1epWVlWXP6du3r3r16qXy8nKNHDlS5eXlGjRokJKTk+05OTk5mjZtmg4cOKCrrrqqyXY8Ho88Ho/93O12S5K8Xq+8Xm9AemstZ6TV/HiE5fNnR9HSz6ulPtoq2H0315+/eruQ1vQd7PeqvzX2E2p9fR36pu9QF8ieW7tOh2VZAfvN3dDQoNtvv111dXV69913JUmlpaX60Y9+5BM2JGnEiBG6+eabtXDhQk2dOlUff/yxNm3aZC8/ffq0unTpog0bNmjMmDFNtlVcXKy5c+c2GS8tLVVsbKyfOwMAAIFw+vRpTZo0SSdOnFBcXFyL8wJ6BCY/P1/79++3w0sgFRUVqbCw0H7udruVlpam7OzsC/4A2sPA4k3NjjsjLM0f3qDZuyPkaXC0c1Ut21+c0+x4S320VbD7bq4/f/V2IRfbd0v7wwRer1cul0ujR49WVFRUsMtpN/RN36EukD03nkH5OgELMAUFBVq/fr3Kysr0zW9+0x5PSUnR2bNnVVdXp4SEBHu8urpaKSkp9pydO3f6rK/xLqXGOedzOp1yOp1NxqOiooL+hvLUX/gfK0+D42vntKeWfl7+rjFYfTfXX3vW0da+r5y9udlxk2657gj/HQYDfYeXcOw7ED23dn1+vwvJsiwVFBRo7dq12rp1q9LT032WDxs2TFFRUdqyZYs9dujQIR07dkyZmZmSpMzMTO3bt081NTX2HJfLpbi4OPXv39/fJQMAAMP4/QhMfn6+SktL9ec//1ldu3ZVVVWVJCk+Pl6dO3dWfHy8Jk+erMLCQiUmJiouLk4PPvigMjMzNXLkSElSdna2+vfvr/vuu08lJSWqqqrSo48+qvz8/GaPsgDhiA/DAxDO/B5gnnvuOUnSTTfd5DO+atUq3X///ZKkZ599VhEREZowYYI8Ho9ycnL061//2p4bGRmp9evXa9q0acrMzFSXLl2Ul5enefPm+btcAABgIL8HmNbc1BQTE6Ply5dr+fLlLc7p3bu3NmzY4M/SgLDF0RoAoYYvcwQAAMYhwAAAAOMQYAAAgHEC/lUCADqu5q6N4boYACbgCAwAADAOAQYAABiHAAMAAIzDNTAAfPCZMQBMwBEYAABgHAIMAAAwDgEGAAAYhwADAACMw0W8AFqlpYt7m8MFvwACjSMwAADAOAQYAABgHAIMAAAwDgEGAAAYh4t4Afhd4wW/zkhLJSOkgcWbdOiJ711w7vm4EBjAhRBgALSLttzFBABfhwADoENqLvBwVAZAI66BAQAAxiHAAAAA43AKCUBY4dQUEBoIMACMd6kXCHMnFGAeAgwAtIBgA3RcBBgAAHBB54f5xs94CiYCDAD4wVd/wbfmA/wAXBoCjB/xQV1AeGjLf+uBumiY01sId9xGDQAAjMMRGABoZ205etLWI7vcJo5w0aEDzPLly/X000+rqqpKQ4YM0bJlyzRiRJCvGgKAEEHYgck6bIB5+eWXVVhYqBUrVigjI0OLFy9WTk6ODh06pKSkpGCXBwDGCNT1eS3dmTKweJM89Y4m8wlH8KcOG2AWLVqkKVOm6Ec/+pEkacWKFXrjjTf0wgsvaNasWUGujgt2AYSm9v7d5o/TaZc6F2bqkAHm7NmzqqioUFFRkT0WERGhrKwslZeXN/saj8cjj8djPz9x4oQkqba2Vl6v1+81djp36tLX0WDp9OkGdfJGqL6h6f+tBMu//vWvZsf90bMU/L6b689fvV1Ie/UdrP6a4++eA/3e9JeL7bsj7buL8XV9t6W/b//8laZzW9jupc69VM4IS49e1aB//etfioqK8lmWsWCL37fXVjuKRjU73lxtLc09fz817uvmer5UX3zxhSTJsqwLT7Q6oE8//dSSZG3fvt1nfMaMGdaIESOafc1jjz1mSeLBgwcPHjx4hMDjk08+uWBW6JBHYC5GUVGRCgsL7ecNDQ2qra1V9+7d5XB0nKMbX+V2u5WWlqZPPvlEcXFxwS6n3dB3+PQdjj1L9E3foS+QPVuWpS+++EKpqakXnNchA0yPHj0UGRmp6upqn/Hq6mqlpKQ0+xqn0ymn0+kzlpCQEKgS/SouLi5s3vRfRd/hIxx7lug73IRj34HqOT4+/mvndMgPsouOjtawYcO0Zcv/Oz/X0NCgLVu2KDMzM4iVAQCAjqBDHoGRpMLCQuXl5Wn48OEaMWKEFi9erFOnTtl3JQEAgPDVYQPMvffeq3/+85+aM2eOqqqqNHToUG3cuFHJycnBLs1vnE6nHnvssSanvkIdfYdP3+HYs0Tf9B36OkLPDsv6uvuUAAAAOpYOeQ0MAADAhRBgAACAcQgwAADAOAQYAABgHAIMAAAwDgGmHZSVlem2225TamqqHA6H1q1b57PcsizNmTNHPXv2VOfOnZWVlaUjR44Ep1g/WbBgga6++mp17dpVSUlJuvPOO3Xo0CGfOWfOnFF+fr66d++uyy67TBMmTGjy6cumee655zR48GD70ykzMzP1l7/8xV4eij2f76mnnpLD4dD06dPtsVDsu7i4WA6Hw+fRt29fe3ko9tzo008/1Q9+8AN1795dnTt31qBBg7R79257eSj+Trv88sub7G+Hw6H8/HxJobu/6+vrNXv2bKWnp6tz58664oorNH/+fJ8vWgza/r70r17E19mwYYP1y1/+0vrTn/5kSbLWrl3rs/ypp56y4uPjrXXr1ll/+9vfrNtvv91KT0+3/v3vfwenYD/IycmxVq1aZe3fv9+qrKy0xo4da/Xq1cs6efKkPefHP/6xlZaWZm3ZssXavXu3NXLkSOuaa64JYtWX7rXXXrPeeOMN6/Dhw9ahQ4esX/ziF1ZUVJS1f/9+y7JCs+ev2rlzp3X55ZdbgwcPth566CF7PBT7fuyxx6wBAwZYn332mf345z//aS8PxZ4ty7Jqa2ut3r17W/fff7+1Y8cO6x//+Ie1adMm68MPP7TnhOLvtJqaGp997XK5LEnWW2+9ZVlW6O7vJ554wurevbu1fv166+jRo9arr75qXXbZZdaSJUvsOcHa3wSYdnZ+gGloaLBSUlKsp59+2h6rq6uznE6n9bvf/S4IFQZGTU2NJcnatm2bZVlf9hgVFWW9+uqr9pyDBw9akqzy8vJglRkQ3bp1s/7v//4v5Hv+4osvrCuvvNJyuVzWjTfeaAeYUO37scces4YMGdLsslDt2bIsa+bMmdZ1113X4vJw+Z320EMPWVdccYXV0NAQ0vt73Lhx1gMPPOAzdtddd1m5ubmWZQV3f3MKKciOHj2qqqoqZWVl2WPx8fHKyMhQeXl5ECvzrxMnTkiSEhMTJUkVFRXyer0+ffft21e9evUKmb7r6+v1+9//XqdOnVJmZmbI95yfn69x48b59CeF9r4+cuSIUlNT9a1vfUu5ubk6duyYpNDu+bXXXtPw4cN19913KykpSVdddZV+85vf2MvD4Xfa2bNn9dJLL+mBBx6Qw+EI6f19zTXXaMuWLTp8+LAk6W9/+5veffddjRkzRlJw93eH/SqBcFFVVSVJTb4iITk52V5muoaGBk2fPl3XXnutBg4cKOnLvqOjo5t8Y3go9L1v3z5lZmbqzJkzuuyyy7R27Vr1799flZWVIdvz73//e73//vvatWtXk2Whuq8zMjK0evVq9enTR5999pnmzp2r66+/Xvv37w/ZniXpH//4h5577jkVFhbqF7/4hXbt2qWf/vSnio6OVl5eXlj8Tlu3bp3q6up0//33Swrd97gkzZo1S263W3379lVkZKTq6+v1xBNPKDc3V1Jw/w0jwCDg8vPztX//fr377rvBLqVd9OnTR5WVlTpx4oT+8Ic/KC8vT9u2bQt2WQHzySef6KGHHpLL5VJMTEywy2k3jf8HKkmDBw9WRkaGevfurVdeeUWdO3cOYmWB1dDQoOHDh+vJJ5+UJF111VXav3+/VqxYoby8vCBX1z6ef/55jRkzRqmpqcEuJeBeeeUVrVmzRqWlpRowYIAqKys1ffp0paamBn1/cwopyFJSUiSpydXq1dXV9jKTFRQUaP369Xrrrbf0zW9+0x5PSUnR2bNnVVdX5zM/FPqOjo7Wt7/9bQ0bNkwLFizQkCFDtGTJkpDtuaKiQjU1Nfrud7+rTp06qVOnTtq2bZuWLl2qTp06KTk5OST7Pl9CQoK+853v6MMPPwzZfS1JPXv2VP/+/X3G+vXrZ58+C/XfaR9//LHefPNN/dd//Zc9Fsr7e8aMGZo1a5YmTpyoQYMG6b777tPDDz+sBQsWSAru/ibABFl6erpSUlK0ZcsWe8ztdmvHjh3KzMwMYmWXxrIsFRQUaO3atdq6davS09N9lg8bNkxRUVE+fR86dEjHjh0zuu/mNDQ0yOPxhGzPo0aN0r59+1RZWWk/hg8frtzcXPvvodj3+U6ePKm///3v6tmzZ8jua0m69tprm3wkwuHDh9W7d29Jofs7rdGqVauUlJSkcePG2WOhvL9Pnz6tiAjfqBAZGamGhgZJQd7fAb1EGJZlfXl3xp49e6w9e/ZYkqxFixZZe/bssT7++GPLsr68BS0hIcH685//bO3du9e64447jL/lcNq0aVZ8fLz19ttv+9x6ePr0aXvOj3/8Y6tXr17W1q1brd27d1uZmZlWZmZmEKu+dLNmzbK2bdtmHT161Nq7d681a9Ysy+FwWJs3b7YsKzR7bs5X70KyrNDs+2c/+5n19ttvW0ePHrX++te/WllZWVaPHj2smpoay7JCs2fL+vJW+U6dOllPPPGEdeTIEWvNmjVWbGys9dJLL9lzQvF3mmVZVn19vdWrVy9r5syZTZaF6v7Oy8uzvvGNb9i3Uf/pT3+yevToYT3yyCP2nGDtbwJMO3jrrbcsSU0eeXl5lmV9eRva7NmzreTkZMvpdFqjRo2yDh06FNyiL1Fz/UqyVq1aZc/597//bf3kJz+xunXrZsXGxlrjx4+3Pvvss+AV7QcPPPCA1bt3bys6Otr6j//4D2vUqFF2eLGs0Oy5OecHmFDs+95777V69uxpRUdHW9/4xjese++91+ezUEKx50avv/66NXDgQMvpdFp9+/a1Vq5c6bM8FH+nWZZlbdq0yZLUbC+hur/dbrf10EMPWb169bJiYmKsb33rW9Yvf/lLy+Px2HOCtb8dlvWVj9MDAAAwANfAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4/x8hoWBnrBCP+QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([len(el) for el in ds['train']['input_ids']])\n",
    "df.hist(bins=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7590070962905884,\n",
       " 'eval_runtime': 0.5791,\n",
       " 'eval_samples_per_second': 86.342,\n",
       " 'eval_steps_per_second': 3.454}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_accuracy():\n",
    "    predictions, _, _ = trainer.predict(ds[\"test\"])\n",
    "\n",
    "    #predictions = scipy.special.softmax(predictions, axis=-1)\n",
    "    #predictions = np.argmax(predictions, axis=-1)\n",
    "    predictions = scipy.special.expit(predictions[:, 0])\n",
    "    predictions = np.where(predictions > 0.5, 0, 1)\n",
    "\n",
    "    references = np.where(np.array(ds['test']['p(Hallucination)']) > 0.5, 0, 1)\n",
    "\n",
    "    accuracy = (predictions == references).sum() / predictions.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0.8]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/385 00:07 < 02:10, 2.78 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(accs)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(get_accuracy())\n\u001b[1;32m     10\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data1/malto/fborra/venv/lib/python3.9/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1541\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/malto/fborra/venv/lib/python3.9/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/data1/malto/fborra/venv/lib/python3.9/site-packages/transformers/trainer.py:2746\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2744\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2746\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/data1/malto/fborra/venv/lib/python3.9/site-packages/accelerate/accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/malto/fborra/venv/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/malto/fborra/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "accs = []\n",
    "for i in range(3):\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    print(accs)\n",
    "    trainer.train()\n",
    "    accs.append(get_accuracy())\n",
    "clear_output(wait=True)\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tgt', 'task', 'src', 'id', 'hyp'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = load_dataset(\"json\", data_files=[str(BASE_DIR / \"test.model-agnostic.json\")])\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_test(examples): # not batched\n",
    "    model_inputs = tokenizer(examples['hyp'], examples['tgt'], truncation=True, max_length=80)\n",
    "    return model_inputs\n",
    "def add_columns(examples):\n",
    "    return {'p(Hallucination)' : 0.01, 'C-W': 1.01}\n",
    "\n",
    "ds_test = ds_test.map(preprocess_function_test).remove_columns(['tgt', 'task', 'src', 'id', 'hyp']).map(add_columns, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "predictions, _, _ = trainer.predict(ds_test[\"train\"])\n",
    "\n",
    "probabilities = scipy.special.expit(predictions[:, 0])\n",
    "predictions = np.where(probabilities > 0.5, \"Hallucination\", \"Not Hallucination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_new = load_dataset(\"json\", data_files=[str(BASE_DIR / \"test.model-agnostic.json\")])\n",
    "\n",
    "global count\n",
    "count = 0\n",
    "def add_predictions(examples):\n",
    "    global count\n",
    "    prob = probabilities[count]\n",
    "    pred = predictions[count]\n",
    "    count += 1\n",
    "    return {'p(Hallucination)' : prob, 'label' : pred}\n",
    "ds_test_new = ds_test_new.map(add_predictions).remove_columns(['tgt', 'task', 'src', 'hyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5002ca0b1b43d7817133bc0e428e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "105669"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test_new['train'].to_json(str(BASE_DIR / \"submission.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "l = [el for el in ds_test_new['train']]\n",
    "with open(BASE_DIR / \"submission.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
