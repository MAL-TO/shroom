{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(texts, model, tokenizer, language=\"fr\"):\n",
    "    # Prepare the text data into appropriate format for the model\n",
    "    template = lambda text: f\"{text}\" if language == \"en\" else f\">>{language}<< {text}\"\n",
    "    src_texts = [template(text) for text in texts]\n",
    "\n",
    "    # Tokenize the texts\n",
    "    encoded = tokenizer.prepare_seq2seq_batch(src_texts,\n",
    "                                              return_tensors='pt')\n",
    "    \n",
    "    # Generate translation using model\n",
    "    translated = model.generate(**encoded)\n",
    "\n",
    "    # Convert the generated tokens indices back into text\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    \n",
    "    return translated_texts\n",
    "\n",
    "def back_translate(texts, target_model, target_tokenizer, source_model, source_tokenizer, target_lang=\"fr\", source_lang=\"en\" ):\n",
    "    # Translate to target language\n",
    "    fr_texts = translate(texts, target_model, target_tokenizer, \n",
    "                         language=target_lang)\n",
    "\n",
    "    # Translate from target language back to source language\n",
    "    back_translated_texts = translate(fr_texts, source_model, source_tokenizer, \n",
    "                                      language=source_lang)\n",
    "    \n",
    "    return back_translated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "target_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
    "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
    "target_model = MarianMTModel.from_pretrained(target_model_name)\n",
    "\n",
    "\n",
    "en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
    "en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
    "en_model = MarianMTModel.from_pretrained(en_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is so cool', 'I hated the food', 'They were very helpful'] \n",
      " ['This is so great.', 'I hated food.', 'They were very helpful.']\n"
     ]
    }
   ],
   "source": [
    "en_texts = ['This is so cool', 'I hated the food', 'They were very helpful']\n",
    "source_lang=\"en\"\n",
    "target_lang=\"es\"\n",
    "\n",
    "aug_texts = back_translate(en_texts, \n",
    "                           target_model=target_model, target_tokenizer=target_tokenizer,\n",
    "                           source_model=en_model, source_tokenizer=en_tokenizer, \n",
    "                           source_lang=source_lang, target_lang=target_lang)\n",
    "print(en_texts,\"\\n\",aug_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
